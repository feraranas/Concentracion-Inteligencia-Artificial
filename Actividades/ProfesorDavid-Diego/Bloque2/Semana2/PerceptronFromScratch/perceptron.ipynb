{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<font size = 8 color ='336EFF'>Perceptron</font>"
      ],
      "metadata": {
        "id": "1W1RAfkc9MHr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b77qOMhq_5iE"
      },
      "source": [
        "Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-MkCVI-_vFJ"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtyIKGW0_3vl"
      },
      "source": [
        "The Class Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4BheILC_yrv"
      },
      "source": [
        "class Perceptron:\n",
        "\n",
        "    def __init__(self, num_inputs, lr, epochs, weights=None):\n",
        "        if weights:\n",
        "            self.weights = weights # if the weights are given as an input\n",
        "        else:\n",
        "            self.weights = np.random.rand(num_inputs+1) # include an extra weight for the bias\n",
        "\n",
        "        self.lr = lr # learning rate\n",
        "        self.epochs = epochs # num de iterations\n",
        "\n",
        "    def act_fn(self, x, fun='step'):\n",
        "        \"\"\"Activation Function \"\"\"\n",
        "        if fun == 'step':\n",
        "            return np.where(x>0, 1, 0)\n",
        "        if fun == 'sigmoid':\n",
        "            return 1/(1 + np.exp(-x))\n",
        "        if fun == 'relu':\n",
        "            return np.where(x>0, x, 0)\n",
        "        if fun == 'tanh':\n",
        "            return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
        "\n",
        "    def predict(self, input):\n",
        "        \"\"\"feedforward perceptron\"\"\"\n",
        "        #return self.act_fn(np.dot(input, self.weights[1:]) + self.weights[0])\n",
        "        return self.act_fn(np.dot(input, self.weights))\n",
        "\n",
        "    def train(self, in_train, classes, verbose=False):\n",
        "        \"\"\"train the perceptron\"\"\"\n",
        "        errors = []\n",
        "        for _ in range(self.epochs):\n",
        "            e = 0\n",
        "            for input, classs in zip(in_train, classes):\n",
        "                prediction = self.predict(input)\n",
        "                error = classs - prediction\n",
        "                #self.weights[0] += self.lr * error # the bias\n",
        "                #self.weights[1:] += self.lr * error * input\n",
        "                self.weights += self.lr * error * input\n",
        "                e += np.abs(error)\n",
        "            errors.append(e)\n",
        "\n",
        "        if verbose:\n",
        "            plt.figure(), plt.plot(errors), plt.title('errors'), plt.show()\n",
        "\n",
        "    def get_weights(self):\n",
        "        \"\"\"get the weights\"\"\"\n",
        "        return self.weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7F9ZhnMBUGX"
      },
      "source": [
        "The idea is to train the perceptron to work as the logic gate AND, which is:\n",
        "\n",
        "| $x_1$  | $x_2$  |   $y$|\n",
        "| ------ |:------:| -:|\n",
        "| 0      | 0      | 0 |\n",
        "| 0      | 1      | 0 |\n",
        "| 1      | 0      | 0 |\n",
        "| 1      | 1      | 1 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2Y9xqCyA0Ra"
      },
      "source": [
        "First create the variables that represent that inputs (inputs $x_1, x_2$, outputs $y$)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EKyDrPIAxpX"
      },
      "source": [
        "x = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "y = np.array([0, 0, 0, 1])\n",
        "\n",
        "print(x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLeR3UJMAzfg"
      },
      "source": [
        "Now initialize the class `Perceptron`and indicate the number of inputs (which correspond to the inputs of the AND table), the learning rate and number of epochs to train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsIKMgojnjOI"
      },
      "source": [
        "perc = Perceptron(num_inputs=2, lr=0.2, epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzqdrJW7n33H"
      },
      "source": [
        "Lets observe how the different activation function works. As inputs they will use values from -1 to 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kx26rHcjnzDZ"
      },
      "source": [
        "t = np.arange(-3, 3, 0.01)\n",
        "#plot each activation function\n",
        "plt.figure()\n",
        "plt.subplot(221), plt.plot(t, perc.act_fn(t, 'step')), plt.title('step')\n",
        "plt.subplot(222), plt.plot(t, perc.act_fn(t, 'sigmoid')), plt.title('sigmoid')\n",
        "plt.subplot(223), plt.plot(t, perc.act_fn(t, 'relu')), plt.title('relu')\n",
        "plt.subplot(224), plt.plot(t, perc.act_fn(t, 'tanh')), plt.title('tanh')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_nNIIyMt2iN"
      },
      "source": [
        "We can observe the initial bias and weights of the perceptron."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfB3BHx9pIZh"
      },
      "source": [
        "print(perc.get_weights())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaPEH_eNvh3n"
      },
      "source": [
        "Using the bias and weights we can make initial prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDvECXl4uKCa"
      },
      "source": [
        "for input, label in zip(x, y):\n",
        "    inp = np.concatenate(([1], input), axis=0)\n",
        "    prediction = perc.predict(inp)\n",
        "    print('input:', input, ' True Label:', label, ' Prediction:', prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZ014iudwZRs"
      },
      "source": [
        "At the moment the prediction is terrible since the perceptron has not been trained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Vmq0qsHwL1E"
      },
      "source": [
        "data = np.concatenate((np.ones((x.shape[0], 1)), x), axis = 1)\n",
        "perc.train(data, y, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbj69d7BxGC5"
      },
      "source": [
        "In the graph we can see the errors for each epoch until convergance(zero errors). Now lets check how the perceptron predicts now that it is trained with the current input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYiTqmV6xAud"
      },
      "source": [
        "for input, label in zip(x, y):\n",
        "    inp = np.concatenate(([1], input), axis=0)\n",
        "    prediction = perc.predict(inp)\n",
        "    print('input:', input, ' True Label:', label, ' Prediction:', prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5UtOCiHx1SP"
      },
      "source": [
        "Success! Now lets get the weights so we don't have to train it again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CPd7-Q-xzfb"
      },
      "source": [
        "perc.get_weights()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBeiliNYyPeF"
      },
      "source": [
        "weights = [-0.49665638,  0.45672912,  0.15160198]\n",
        "\n",
        "perc2 = Perceptron(2, 0.2, 0, weights=weights)\n",
        "\n",
        "print('perceptron\\'s weights: ', perc2.get_weights())\n",
        "\n",
        "for input, label in zip(x, y):\n",
        "    inp = np.concatenate(([1], input), axis=0)\n",
        "    prediction = perc2.predict(inp)\n",
        "    print('input:', input, ' True Label:', label, ' Prediction:', prediction)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQs15dcayn56"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}